{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielJobert/Simulation_paper---Effect_of_missing_data_on_K-means_performance---MATH60603A_STATISTICAL_LEARNING/blob/main/M%C3%A9moire_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librairies"
      ],
      "metadata": {
        "id": "QOmNGg924mJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of libraries to install\n",
        "libraries = ['torch', 'transformers', 'datasets', 'torchtext', 'faiss-cpu', 'rank_bm25', 'git+https://github.com/stanford-futuredata/ColBERT.git']\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        # Attempt to install the library\n",
        "        !pip install {lib}\n",
        "        print(f'Successfully installed {lib}')\n",
        "    except Exception as e:\n",
        "        # Catch and print any errors\n",
        "        print(f'An error occurred while installing {lib}: {e}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4St723nNHdmz",
        "outputId": "e2d760fe-efa8-4cca-ee53-cbcbfd529149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Successfully installed torch\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Successfully installed transformers\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Successfully installed datasets\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Successfully installed torchtext\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Successfully installed faiss-cpu\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Successfully installed rank_bm25\n",
            "Collecting git+https://github.com/stanford-futuredata/ColBERT.git\n",
            "  Cloning https://github.com/stanford-futuredata/ColBERT.git to /tmp/pip-req-build-q08efapl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stanford-futuredata/ColBERT.git /tmp/pip-req-build-q08efapl\n",
            "  Resolved https://github.com/stanford-futuredata/ColBERT.git to commit 85837b6af6f92bbdd13238fbaa6f99f2f073da8e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (3.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (3.0.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (2.2.5)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (1.0.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (1.11.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (4.66.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (4.44.2)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->colbert-ai==0.2.20) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (8.1.7)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from git-python->colbert-ai==0.2.20) (3.1.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.20) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.20) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.20) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets->colbert-ai==0.2.20) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->colbert-ai==0.2.20) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (2024.8.30)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->git-python->colbert-ai==0.2.20) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.20) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.20) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.20) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.20) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->colbert-ai==0.2.20) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->colbert-ai==0.2.20) (0.2.0)\n",
            "Successfully installed git+https://github.com/stanford-futuredata/ColBERT.git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE1OA2N42VTC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import faiss\n",
        "from abc import ABC, abstractmethod\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import AutoModel, DPRQuestionEncoder, DPRContextEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "ADk2g8kw4qCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_obliqa_dataset():\n",
        "    # Define the URL of the dataset file\n",
        "    url = \"https://raw.githubusercontent.com/RegNLP/ObliQADataset/main/ObliQA_train.json\"\n",
        "\n",
        "    # Define the local path where the file will be saved\n",
        "    local_path = \"/content/ObliQADataset-main/ObliQA_train.json\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "    # Download the dataset\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Raise an error if the request was unsuccessful\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save the file\n",
        "    with open(local_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"Dataset downloaded successfully and saved to {local_path}\")\n",
        "\n",
        "# Run the function\n",
        "download_obliqa_dataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_499qvJzgOQ",
        "outputId": "9aaf7513-76cb-4a8b-cfa3-a660894ac01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded successfully and saved to /content/ObliQADataset-main/ObliQA_train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "\n",
        "class DatasetLoader(Dataset):\n",
        "    \"\"\"\n",
        "    General class to handle loading, preprocessing, and tokenization of various datasets for model training.\n",
        "    Supports multiple datasets and can be extended to add more in the future.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name: str, version: str = None, split: str = 'train',\n",
        "                 tokenizer_name: str = 'bert-base-uncased', max_length: int = 384,\n",
        "                 dataset_path: str = None, include_negatives: bool = False):\n",
        "        \"\"\"\n",
        "        Initializes the DatasetLoader with dataset name, split, tokenizer, and preprocessing options.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): Name of the dataset (e.g., 'squad', 'ms_marco', 'obliqa').\n",
        "            version (str, optional): Specific version of the dataset (if applicable).\n",
        "            split (str, optional): Dataset split to load ('train', 'validation', etc.).\n",
        "            tokenizer_name (str, optional): Name of the tokenizer to use (e.g., 'bert-base-uncased').\n",
        "            max_length (int, optional): Maximum length for tokenized inputs.\n",
        "            dataset_path (str, optional): Path to the dataset file (for custom datasets like 'obliqa').\n",
        "            include_negatives (bool, optional): Whether to include negative samples.\n",
        "        \"\"\"\n",
        "        self.dataset_name = dataset_name\n",
        "        self.version = version\n",
        "        self.split = split\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_length = max_length\n",
        "        self.include_negatives = include_negatives\n",
        "        self.dataset_path = dataset_path\n",
        "        self.dataset = self._load_dataset()\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads the specified dataset dynamically based on the dataset name.\n",
        "\n",
        "        Returns:\n",
        "            Dataset: The loaded dataset or list of data samples.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.dataset_name == 'squad':\n",
        "                return load_dataset('squad_v2' if self.version == '2.0' else 'squad', split=self.split)\n",
        "            elif self.dataset_name == 'ms_marco':\n",
        "                return load_dataset('ms_marco', self.version, split=self.split)\n",
        "            elif self.dataset_name == 'obliqa':\n",
        "                return self._load_obliqa_dataset(self.dataset_path)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported dataset: {self.dataset_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_obliqa_dataset(self, dataset_path: str):\n",
        "        \"\"\"\n",
        "        Loads the ObliQA dataset from a JSON file.\n",
        "\n",
        "        Args:\n",
        "            dataset_path (str): Path to the ObliQA dataset file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of loaded dataset samples.\n",
        "        \"\"\"\n",
        "        with open(dataset_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves an item from the dataset, tokenizes the query and passage,\n",
        "        and prepares data for the model. Optionally includes negative sampling.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing tokenized inputs for the model.\n",
        "        \"\"\"\n",
        "        data = self.dataset[idx]\n",
        "\n",
        "        if self.dataset_name == 'squad':\n",
        "            return self._process_squad(data, idx)\n",
        "        elif self.dataset_name == 'ms_marco':\n",
        "            return self._process_ms_marco(data, idx)\n",
        "        elif self.dataset_name == 'obliqa':\n",
        "            return self._process_obliqa(data, idx)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset: {self.dataset_name}\")\n",
        "\n",
        "    def _process_squad(self, data, idx):\n",
        "        # Tokenize the context (passage) and question for the SQuAD dataset\n",
        "        passage_inputs = self.tokenizer(\n",
        "            data['context'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        query_inputs = self.tokenizer(\n",
        "            data['question'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.include_negatives:\n",
        "            neg_idx = (idx + 1) % len(self.dataset)\n",
        "            negative_data = self.dataset[neg_idx]\n",
        "            negative_inputs = self.tokenizer(\n",
        "                negative_data['context'],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                'query_input_ids': query_inputs['input_ids'].squeeze(),\n",
        "                'passage_input_ids': passage_inputs['input_ids'].squeeze(),\n",
        "                'negative_input_ids': negative_inputs['input_ids'].squeeze()\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'query_input_ids': query_inputs['input_ids'].squeeze(),\n",
        "            'passage_input_ids': passage_inputs['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "    def _process_ms_marco(self, data, idx):\n",
        "        # Tokenize the query and passage for the MS MARCO dataset\n",
        "        query_inputs = self.tokenizer(\n",
        "            data['query'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        selected_passages = [\n",
        "            passage for passage, is_selected in zip(data['passages']['passage_text'], data['passages']['is_selected'])\n",
        "            if is_selected == 1\n",
        "        ] or [data['passages']['passage_text'][0]]  # Fallback\n",
        "\n",
        "        passage_inputs = self.tokenizer(\n",
        "            selected_passages[0],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'query_input_ids': query_inputs['input_ids'].squeeze(),\n",
        "            'passage_input_ids': passage_inputs['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "    def _process_obliqa(self, data, idx):\n",
        "        # Tokenize the question and passage for the ObliQA dataset\n",
        "        query_inputs = self.tokenizer(\n",
        "            data['Question'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        passage_inputs = self.tokenizer(\n",
        "            data['Passages'][0]['Passage'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'query_input_ids': query_inputs['input_ids'].squeeze(),\n",
        "            'passage_input_ids': passage_inputs['input_ids'].squeeze(),\n",
        "            'question_id': data['QuestionID'],\n",
        "            'group': data['Group']\n",
        "        }\n",
        "\n",
        "    def get_dataloader(self, batch_size: int = 8, shuffle: bool = True):\n",
        "        \"\"\"\n",
        "        Creates a DataLoader for the dataset.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int, optional): Number of samples per batch.\n",
        "            shuffle (bool, optional): Whether to shuffle the data.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: A DataLoader instance for the dataset.\n",
        "        \"\"\"\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle)\n"
      ],
      "metadata": {
        "id": "-MRd12wdx969"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Testing with SQuAD v2.0\n",
        "    print(\"Testing SQuAD v2.0 Dataset\")\n",
        "    try:\n",
        "        squad_dataset_loader = DatasetLoader(\n",
        "            dataset_name='squad',\n",
        "            version='2.0',\n",
        "            split='train',\n",
        "            include_negatives=True\n",
        "        )\n",
        "        squad_dataloader = squad_dataset_loader.get_dataloader(batch_size=8, shuffle=True)\n",
        "\n",
        "        for batch in squad_dataloader:\n",
        "            print(\"SQuAD v2.0 Batch:\", batch)\n",
        "            break  # Print only one batch for testing\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing SQuAD v2.0: {e}\")\n",
        "\n",
        "    # Example 2: Testing with MS MARCO v2.1\n",
        "    print(\"\\nTesting MS MARCO v2.1 Dataset\")\n",
        "    try:\n",
        "        ms_marco_dataset_loader = DatasetLoader(\n",
        "            dataset_name='ms_marco',\n",
        "            version='v2.1',\n",
        "            split='train',\n",
        "            include_negatives=True\n",
        "        )\n",
        "        ms_marco_dataloader = ms_marco_dataset_loader.get_dataloader(batch_size=8, shuffle=True)\n",
        "\n",
        "        for batch in ms_marco_dataloader:\n",
        "            print(\"MS MARCO v2.1 Batch:\", batch)\n",
        "            break  # Print only one batch for testing\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing MS MARCO v2.1: {e}\")\n",
        "\n",
        "    # Example 3: Testing with ObliQA Dataset\n",
        "    print(\"\\nTesting ObliQA Dataset\")\n",
        "    try:\n",
        "        dataset_path = '/content/ObliQADataset-main/ObliQA_train.json'  # Adjust the path to your dataset file\n",
        "        obliqa_dataset_loader = DatasetLoader(\n",
        "            dataset_name='obliqa',\n",
        "            dataset_path=dataset_path,\n",
        "            include_negatives=True\n",
        "        )\n",
        "        obliqa_dataloader = obliqa_dataset_loader.get_dataloader(batch_size=8, shuffle=True)\n",
        "\n",
        "        for batch in obliqa_dataloader:\n",
        "            print(\"ObliQA Batch:\", batch)\n",
        "            break  # Print only one batch for testing\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing ObliQA: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IB252vdz6BK",
        "outputId": "abab8b8a-91a1-4454-c1e5-eb659d2ab8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing SQuAD v2.0 Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQuAD v2.0 Batch: {'query_input_ids': tensor([[ 101, 2043, 2020,  ...,    0,    0,    0],\n",
            "        [ 101, 1997, 2054,  ...,    0,    0,    0],\n",
            "        [ 101, 7937, 3514,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2247, 2007,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 4277,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 2001,  ...,    0,    0,    0]]), 'passage_input_ids': tensor([[  101,  3199,  2810,  ...,     0,     0,     0],\n",
            "        [  101,  3025,  2695,  ...,     0,     0,     0],\n",
            "        [  101, 16295,  1013,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2116,  1997,  ...,     0,     0,     0],\n",
            "        [  101,  1999,  2494,  ...,     0,     0,     0],\n",
            "        [  101,  2522, 22145,  ...,     0,     0,     0]]), 'negative_input_ids': tensor([[  101,  3199,  2810,  ...,     0,     0,     0],\n",
            "        [  101,  3025,  2695,  ...,     0,     0,     0],\n",
            "        [  101, 16295,  1013,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2116,  1997,  ...,     0,     0,     0],\n",
            "        [  101,  1999,  2494,  ...,     0,     0,     0],\n",
            "        [  101,  2522, 22145,  ...,     0,     0,     0]])}\n",
            "\n",
            "Testing MS MARCO v2.1 Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MS MARCO v2.1 Batch: {'query_input_ids': tensor([[  101,  3517,  3465,  ...,     0,     0,     0],\n",
            "        [  101,  2515,  1037,  ...,     0,     0,     0],\n",
            "        [  101,  2054,  9983,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2054,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  2054,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  2003, 19857,  ...,     0,     0,     0]]), 'passage_input_ids': tensor([[  101,  1996,  5725,  ...,     0,     0,     0],\n",
            "        [  101, 28079,  7395,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  3227,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  3078,  8040,  ...,     0,     0,     0],\n",
            "        [  101,  2004, 10288,  ...,     0,     0,     0],\n",
            "        [  101,  2619,  2007,  ...,     0,     0,     0]])}\n",
            "\n",
            "Testing ObliQA Dataset\n",
            "ObliQA Batch: {'query_input_ids': tensor([[ 101, 2071, 2017,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
            "        [ 101, 1999, 1996,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 2024,  ...,    0,    0,    0]]), 'passage_input_ids': tensor([[  101,  2592,  1998,  ...,     0,     0,     0],\n",
            "        [  101,  3627,  1019,  ...,     0,     0,     0],\n",
            "        [  101, 10738,  5918,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2019, 16021,  ...,     0,     0,     0],\n",
            "        [  101,  5433,  1023,  ...,     0,     0,     0],\n",
            "        [  101,  1999,  2804,  ...,     0,     0,     0]]), 'question_id': ['b518d50f-b1bf-4017-bc56-18162ffc76ad', 'f57443f2-717e-4fbd-a547-5f3043315dff', '23a3e9f6-8850-4a7e-85b5-04eb1f20cd2b', 'd96b9c54-e7a5-407b-97e7-c55e5fb4aa2b', '340c5443-2f52-4ade-a98f-2331414c6e76', 'd7a293c1-cb73-458f-aace-572cbe6af9ce', '3cc4208a-1c28-47e7-b325-4fd5fe04dded', 'ad006a7a-d266-4780-aa31-39726fcaa94f'], 'group': tensor([3, 3, 4, 2, 1, 1, 1, 4])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "bC39sb-h42Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "\n",
        "class Retriever(ABC):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode_passages(self, passages):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def retrieve(self, query, index, passages, top_k):\n",
        "        pass\n",
        "\n",
        "class LanguageModel(ABC):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, input_text):\n",
        "        pass\n",
        "\n",
        "class ContrieverRetriever(Retriever):\n",
        "    def encode_passages(self, passages, batch_size=16):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def retrieve(self, query, index, passages, top_k=5):\n",
        "        query_inputs = self.tokenizer(query, return_tensors='pt').to(self.device)\n",
        "        with torch.no_grad():\n",
        "            query_embedding = self.model(**query_inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        return [passages[i] for i in indices[0]]\n",
        "\n",
        "class FlanT5LanguageModel(LanguageModel):\n",
        "    def generate(self, input_text):\n",
        "        input_ids = self.tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).input_ids.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(input_ids, max_length=256)\n",
        "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, retriever, language_model, passages, index_file='passage_index.faiss'):\n",
        "        self.retriever = retriever\n",
        "        self.language_model = language_model\n",
        "        self.passages = passages\n",
        "        self.index_file = index_file\n",
        "        self.index = self._load_index() or self._build_index(passages)\n",
        "\n",
        "    def _build_index(self, passages, batch_size=16):\n",
        "        passage_embeddings = self.retriever.encode_passages(passages, batch_size)\n",
        "        dimension = passage_embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "        index.add(passage_embeddings)\n",
        "        faiss.write_index(index, self.index_file)\n",
        "        return index\n",
        "\n",
        "    def _load_index(self):\n",
        "        try:\n",
        "            return faiss.read_index(self.index_file)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def __call__(self, query, top_k=5):\n",
        "        retrieved_passages = self.retriever.retrieve(query, self.index, self.passages, top_k)\n",
        "        input_text = \" \".join(retrieved_passages[:top_k]) + \" \" + query\n",
        "        answer = self.language_model.generate(input_text)\n",
        "        return answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a subset of the dataset with only 10 items\n",
        "    dataset = load_dataset('squad_v2', split='train[:10]')\n",
        "    passages = [item['context'] for item in dataset]\n",
        "\n",
        "    # Instantiate retriever and language model\n",
        "    retriever = ContrieverRetriever(model_name='facebook/contriever')\n",
        "    language_model = FlanT5LanguageModel(model_name='google/flan-t5-base')\n",
        "\n",
        "    # Create the RAG pipeline\n",
        "    pipeline = RAGPipeline(retriever=retriever, language_model=language_model, passages=passages)\n",
        "\n",
        "    # Test the pipeline\n",
        "    query = \"What is the capital of France?\"\n",
        "    result = pipeline(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Generated Answer: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p36Rshgth8EF",
        "outputId": "ab68a827-0a69-4415-dc22-52df68271d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the capital of France?\n",
            "Generated Answer: -selling girl groups of all time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "# Base Retriever class\n",
        "class Retriever(ABC):\n",
        "    def __init__(self, model_name=None):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) if model_name else None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device) if model_name else None\n",
        "        if self.device == 'cuda' and self.model:\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode_passages(self, passages):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def retrieve(self, query, index, passages, top_k):\n",
        "        pass\n",
        "\n",
        "# BM25-based Retriever\n",
        "class BM25Retriever(Retriever):\n",
        "    def __init__(self):\n",
        "        super().__init__(None)\n",
        "        self.bm25 = None\n",
        "\n",
        "    def encode_passages(self, passages):\n",
        "        # Tokenize passages for BM25\n",
        "        tokenized_passages = [passage.split() for passage in passages]\n",
        "        self.bm25 = BM25Okapi(tokenized_passages)\n",
        "\n",
        "    def retrieve(self, query, index=None, passages=None, top_k=5):\n",
        "        # Tokenize the query for BM25\n",
        "        tokenized_query = query.split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "        return [passages[i] for i in top_k_indices]\n",
        "\n",
        "# Contriever-based Retriever (already implemented)\n",
        "class ContrieverRetriever(Retriever):\n",
        "    def encode_passages(self, passages, batch_size=16):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def retrieve(self, query, index, passages, top_k=5):\n",
        "        query_inputs = self.tokenizer(query, return_tensors='pt').to(self.device)\n",
        "        with torch.no_grad():\n",
        "            query_embedding = self.model(**query_inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        return [passages[i] for i in indices[0]]\n",
        "\n",
        "# Language Model base class\n",
        "class LanguageModel(ABC):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, input_text):\n",
        "        pass\n",
        "\n",
        "# FlanT5-based language model\n",
        "class FlanT5LanguageModel(LanguageModel):\n",
        "    def generate(self, input_text):\n",
        "        input_ids = self.tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).input_ids.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(input_ids, max_length=256)\n",
        "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# RAG pipeline class\n",
        "class RAGPipeline:\n",
        "    def __init__(self, retriever, language_model, passages, index_file='passage_index.faiss'):\n",
        "        self.retriever = retriever\n",
        "        self.language_model = language_model\n",
        "        self.passages = passages\n",
        "        self.index_file = index_file\n",
        "        self.index = self._load_index() if isinstance(retriever, ContrieverRetriever) else None\n",
        "        if not self.index and isinstance(retriever, ContrieverRetriever):\n",
        "            self.index = self._build_index(passages)\n",
        "\n",
        "    def _build_index(self, passages, batch_size=16):\n",
        "        passage_embeddings = self.retriever.encode_passages(passages, batch_size)\n",
        "        dimension = passage_embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "        index.add(passage_embeddings)\n",
        "        faiss.write_index(index, self.index_file)\n",
        "        return index\n",
        "\n",
        "    def _load_index(self):\n",
        "        try:\n",
        "            return faiss.read_index(self.index_file)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def __call__(self, query, top_k=5):\n",
        "        retrieved_passages = self.retriever.retrieve(query, self.index, self.passages, top_k)\n",
        "        input_text = \" \".join(retrieved_passages[:top_k]) + \" \" + query\n",
        "        answer = self.language_model.generate(input_text)\n",
        "        return answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a subset of the dataset with only 10 items\n",
        "    dataset = load_dataset('squad_v2', split='train[:10]')\n",
        "    passages = [item['context'] for item in dataset]\n",
        "\n",
        "    # Select BM25 retriever or Contriever retriever\n",
        "    use_bm25 = True  # Toggle this to switch retrievers\n",
        "    if use_bm25:\n",
        "        retriever = BM25Retriever()\n",
        "        retriever.encode_passages(passages)\n",
        "    else:\n",
        "        retriever = ContrieverRetriever(model_name='facebook/contriever')\n",
        "\n",
        "    # Instantiate the language model\n",
        "    language_model = FlanT5LanguageModel(model_name='google/flan-t5-base')\n",
        "\n",
        "    # Create the RAG pipeline\n",
        "    pipeline = RAGPipeline(retriever=retriever, language_model=language_model, passages=passages)\n",
        "\n",
        "    # Test the pipeline\n",
        "    query = \"What is the capital of France?\"\n",
        "    result = pipeline(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Generated Answer: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfnEEbwutTLs",
        "outputId": "295cde6e-17c9-4f5f-9ae0-7afaf4ef7fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the capital of France?\n",
            "Generated Answer: -selling girl groups of all time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, DPRQuestionEncoder, DPRContextEncoder\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from colbert.modeling.colbert import ColBERT\n",
        "from colbert.utils.runs import Run\n",
        "from colbert.data import Collection, Queries\n",
        "from colbert.infra import ColBERTConfig\n",
        "from colbert.utils.utils import print_message\n",
        "\n",
        "\n",
        "# Base Retriever class\n",
        "class Retriever(ABC):\n",
        "    def __init__(self, model_name=None):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) if model_name else None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device) if model_name else None\n",
        "        if self.device == 'cuda' and self.model:\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode_passages(self, passages):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def retrieve(self, query, index, passages, top_k):\n",
        "        pass\n",
        "\n",
        "# BM25-based Retriever\n",
        "class BM25Retriever(Retriever):\n",
        "    def __init__(self):\n",
        "        super().__init__(None)\n",
        "        self.bm25 = None\n",
        "\n",
        "    def encode_passages(self, passages):\n",
        "        # Tokenize passages for BM25\n",
        "        tokenized_passages = [passage.split() for passage in passages]\n",
        "        self.bm25 = BM25Okapi(tokenized_passages)\n",
        "\n",
        "    def retrieve(self, query, index=None, passages=None, top_k=5):\n",
        "        # Tokenize the query for BM25\n",
        "        tokenized_query = query.split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "        return [passages[i] for i in top_k_indices]\n",
        "\n",
        "# Contriever-based Retriever (already implemented)\n",
        "class ContrieverRetriever(Retriever):\n",
        "    def encode_passages(self, passages, batch_size=16):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def retrieve(self, query, index, passages, top_k=5):\n",
        "        query_inputs = self.tokenizer(query, return_tensors='pt').to(self.device)\n",
        "        with torch.no_grad():\n",
        "            query_embedding = self.model(**query_inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        return [passages[i] for i in indices[0]]\n",
        "\n",
        "# DPR-based Retriever\n",
        "class DPRRetriever(Retriever):\n",
        "    def __init__(self):\n",
        "        super().__init__('facebook/dpr-question_encoder-single-nq-base')  # Initialize with the question encoder tokenizer\n",
        "        self.query_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(self.device)\n",
        "        self.passage_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(self.device)\n",
        "        # We also need a tokenizer for encoding the passages and queries\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "\n",
        "    def encode_passages(self, passages, batch_size=16):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.passage_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def retrieve(self, query, index, passages, top_k=5):\n",
        "        query_inputs = self.tokenizer(query, return_tensors='pt').to(self.device)\n",
        "        with torch.no_grad():\n",
        "            query_embedding = self.query_encoder(**query_inputs).pooler_output.cpu().numpy()\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        return [passages[i] for i in indices[0]]\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.nn as nn\n",
        "\n",
        "class ColBERTRetriever(Retriever):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
        "        super().__init__(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Initialize linear layer for ColBERT's token interaction\n",
        "        self.linear = nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
        "\n",
        "    def encode(self, texts):\n",
        "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model(**inputs).last_hidden_state\n",
        "        return self.linear(outputs)  # Apply the linear layer for late interaction\n",
        "\n",
        "    def retrieve(self, query, documents, top_k=3):\n",
        "        query_embeds = self.encode(query).mean(dim=1)  # Simplified\n",
        "        doc_embeds = self.encode(documents).mean(dim=1)\n",
        "\n",
        "        scores = torch.matmul(query_embeds, doc_embeds.T)\n",
        "        ranked_indices = torch.argsort(scores[0], descending=True)[:top_k]\n",
        "        return [documents[i] for i in ranked_indices]\n",
        "\n",
        "\n",
        "# Language Model base class\n",
        "class LanguageModel(ABC):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, input_text):\n",
        "        pass\n",
        "\n",
        "# FlanT5-based language model\n",
        "class FlanT5LanguageModel(LanguageModel):\n",
        "    def generate(self, input_text):\n",
        "        input_ids = self.tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).input_ids.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(input_ids, max_length=256)\n",
        "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# RAG pipeline class\n",
        "class RAGPipeline:\n",
        "    def __init__(self, retriever, language_model, passages, index_file='passage_index.faiss'):\n",
        "        self.retriever = retriever\n",
        "        self.language_model = language_model\n",
        "        self.passages = passages\n",
        "        self.index_file = index_file\n",
        "        self.index = self._load_index() if isinstance(retriever, (ContrieverRetriever, DPRRetriever, ColBERTRetriever)) else None\n",
        "        if not self.index and isinstance(retriever, (ContrieverRetriever, DPRRetriever, ColBERTRetriever)):\n",
        "            self.index = self._build_index(passages)\n",
        "\n",
        "    def _build_index(self, passages, batch_size=16):\n",
        "        passage_embeddings = self.retriever.encode_passages(passages, batch_size)\n",
        "        dimension = passage_embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "        index.add(passage_embeddings)\n",
        "        faiss.write_index(index, self.index_file)\n",
        "        return index\n",
        "\n",
        "    def _load_index(self):\n",
        "        try:\n",
        "            return faiss.read_index(self.index_file)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def __call__(self, query, top_k=5):\n",
        "        retrieved_passages = self.retriever.retrieve(query, self.index, self.passages, top_k)\n",
        "        input_text = \" \".join(retrieved_passages[:top_k]) + \" \" + query\n",
        "        answer = self.language_model.generate(input_text)\n",
        "        return answer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a subset of the dataset with only 10 items\n",
        "    dataset = load_dataset('squad_v2', split='train[:10]')\n",
        "    passages = [item['context'] for item in dataset]\n",
        "\n",
        "    # Select BM25, DPR, Contriever, or ColBERT retriever\n",
        "    use_bm25 = False  # Toggle to switch between retrievers\n",
        "    use_dpr = False   # Toggle to switch between retrievers\n",
        "    use_colbert = True  # Toggle to use ColBERT retriever\n",
        "\n",
        "    if use_bm25:\n",
        "        retriever = BM25Retriever()\n",
        "        retriever.encode_passages(passages)\n",
        "    elif use_dpr:\n",
        "        retriever = DPRRetriever()\n",
        "    elif use_colbert:\n",
        "        retriever = ColBERTRetriever(colbert_model_path='bert-base-uncased')  # Provide ColBERT path\n",
        "    else:\n",
        "        retriever = ContrieverRetriever(model_name='facebook/contriever')\n",
        "\n",
        "    # Instantiate the language model\n",
        "    language_model = FlanT5LanguageModel(model_name='google/flan-t5-base')\n",
        "\n",
        "    # Create the RAG pipeline\n",
        "    pipeline = RAGPipeline(retriever=retriever, language_model=language_model, passages=passages)\n",
        "\n",
        "    # Test the pipeline\n",
        "    query = \"What is the capital of France?\"\n",
        "    result = pipeline(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Generated Answer: {result}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "vCcjw4jQvbMR",
        "outputId": "f4a00090-ed81-4d74-d9e3-f217e2b217fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Can't instantiate abstract class ColBERTRetriever with abstract method encode_passages",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0b2104b59036>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPRRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0muse_colbert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColBERTRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolbert_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Provide ColBERT path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContrieverRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'facebook/contriever'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class ColBERTRetriever with abstract method encode_passages"
          ]
        }
      ]
    }
  ]
}